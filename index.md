---
layout: default
---

# About Me

Yuheng Ji (冀昱衡) is now pursuing his bachelor degree in <a href="http://jsjytx.neuq.edu.cn/">School of Computer and Communication Engineering</a>, NUEQ. 
His research interests include Natural Language Processing, Machine Translataion and Dialogue System.

# News

Coming soon...


# Publication

### Journals: 

Coming soon...


### Conferences: 

Coming soon...

### Under Review:

- **<font color="#0000FF">[Under Review]</font>** Miao Fang, **Yuheng Ji**, Heng Cai, Longyan Hong, Hongkai Liu: A Window Mechanism based BERT for Language Understanding in Multi-Turn Dialogue.

# Research Interest

- Natural Language Processing
- Machine Translataion
- Dialogue System

# Education

- **[2019.9-now]** 

   <a href="https://www.neuq.edu.cn/">NorthEastern University at Qinhuangdao</a> (985&211, Double World-Class) - Computer Science and Technology - B.E.

  - GPA: 4.33 / 5.00
  - Rank: 3 / 207 (Top 1.44%)
  - CET-4: 582 / CET-6: 474
  - Main Course: 
    - Probability and Mathematical Statistics (99)
    - Data Structure (98)
    - Academic Paper Reading and Writing (97)
    - Advanced Mathematics (96, 95)
    - Linear Algebra (94)
    - C++ Programming (98)
    - Discrete mathematics (97)
    - Introduction to Artificial Intelligence (97)
    - Principles of Database (97)
    - Compiler Principle (95)
    - Operating System (97)
    - Computer Network (97)
    - Principles of Computer Composition (96)
    - The Design and Analysis of Algorithm (94)
  - Skill:
    - Python / C++
    - Pytorch
  - Self-study:
    - CS224n: Natural Language Processing with Deep Learning

# Academic Experience

- **A Window Mechanism based BERT for Language Understanding in Multi-Turn Dialogue**
  - I participated in the method design and paper writing in this work. 
  - In order to solve the problem of how to effectively use context in multi-turn dialogues, this paper proposes a method which innovatively integrates the window mechanism into pre-traing model BERT. At the same time, for each historical context, the sequence is reversed before fed into the pre-training model. 
  - Experimental results show that the our method is effective. In the two tasks of intention detection and slot filling, the performance of the model on KVRET, MultiWOZ and DSTC8 greatly outperforms joint-Bert, the strong baseline model at that time. 
  - Discussion: improvement of window mechanism(for example, converting static to dynamic), selection of pre-training model(such as TOD-BERT, GALAXY) and so on.
- **Asynchronous and Segmented Bidirectional Decoding for NMT**
  - Based on the thinking of classical Seq2Seq structure and inspired by BIFT, ABD-NMT and other works, I independently proposed a neural machine translation model based on asynchronous multi-segment bi-directional decoding. 
  - At present, l am designing experiments, running experiments and writing papers.

# Award

- **[2022]** **National Scholarship**, Ministry of Education of the People’s Republic of China.
- **[2022]** **Merit Student of Hebei Province**.
- **[2021]** **National Scholarship**, Ministry of Education of the People’s Republic of China.
- **[2020]** **National Scholarship**, Ministry of Education of the People’s Republic of China.


# Friendship Link

- [Yue Liu](https://yueliu1999.github.io/)

